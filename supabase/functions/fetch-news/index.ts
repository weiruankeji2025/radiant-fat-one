import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
}

// æ–°é—»æºé…ç½® - æ‰©å±•ç‰ˆæœ¬
const NEWS_SOURCES = [
  // æ”¿æ²»æ–°é—»
  { url: 'https://www.bbc.com/news/world', name: 'BBC World', category: 'politics' },
  { url: 'https://www.reuters.com/world/', name: 'Reuters', category: 'politics' },
  { url: 'https://www.politico.com/', name: 'Politico', category: 'politics' },
  
  // å›½é™…æ–°é—»
  { url: 'https://www.aljazeera.com/news/', name: 'Al Jazeera', category: 'world' },
  { url: 'https://www.dw.com/en/top-stories/s-9097', name: 'DW News', category: 'world' },
  { url: 'https://www.france24.com/en/', name: 'France24', category: 'world' },
  
  // ç§‘æŠ€æ–°é—»
  { url: 'https://techcrunch.com/', name: 'TechCrunch', category: 'technology' },
  { url: 'https://www.theverge.com/', name: 'The Verge', category: 'technology' },
  { url: 'https://arstechnica.com/', name: 'Ars Technica', category: 'technology' },
  { url: 'https://www.wired.com/', name: 'Wired', category: 'technology' },
  
  // å†›äº‹æ–°é—»
  { url: 'https://www.defense.gov/News/', name: 'Defense.gov', category: 'military' },
  { url: 'https://www.defensenews.com/', name: 'Defense News', category: 'military' },
  { url: 'https://www.janes.com/defence-news', name: 'Janes', category: 'military' },
  
  // ç»æµæ–°é—»
  { url: 'https://www.bloomberg.com/markets', name: 'Bloomberg', category: 'economy' },
  { url: 'https://www.cnbc.com/world/', name: 'CNBC', category: 'economy' },
  { url: 'https://www.ft.com/', name: 'Financial Times', category: 'economy' },
  
  // ä¸­ç¾å…³ç³»
  { url: 'https://www.scmp.com/topics/china-us-relations', name: 'SCMP China-US', category: 'china_us' },
  { url: 'https://www.reuters.com/world/china/', name: 'Reuters China', category: 'china_us' },
  
  // CNMDä¸“æ  - ä¸“é—¨æŠ“å–cnmdnews.comå…¨éƒ¨å†…å®¹
  { url: 'https://www.cnmdnews.com/', name: 'CNMD News', category: 'cnmd' },
  
  // WeiRuanKeJiä¸“æ  - å•ç‹¬æŠ“å–weiruan.orgå…¨éƒ¨å†…å®¹
  { url: 'https://weiruan.org/', name: 'WeiRuanKeJi', category: 'weiruan' },
  
  // ä¿„ä¹Œå†²çª
  { url: 'https://www.reuters.com/world/europe/', name: 'Reuters Europe', category: 'russia_ukraine' },
  { url: 'https://www.bbc.com/news/world/europe', name: 'BBC Europe', category: 'russia_ukraine' },
  { url: 'https://www.aljazeera.com/tag/ukraine-russia-crisis/', name: 'Al Jazeera Ukraine', category: 'russia_ukraine' },
  
  // æœéŸ©åŠå²›
  { url: 'https://www.nknews.org/', name: 'NK News', category: 'korea' },
  { url: 'https://en.yna.co.kr/', name: 'Yonhap News', category: 'korea' },
  { url: 'https://www.koreaherald.com/', name: 'Korea Herald', category: 'korea' },
  
  // ä¸œå—äºš
  { url: 'https://www.channelnewsasia.com/asia', name: 'CNA Asia', category: 'southeast_asia' },
  { url: 'https://www.straitstimes.com/asia', name: 'Straits Times', category: 'southeast_asia' },
  { url: 'https://www.bangkokpost.com/', name: 'Bangkok Post', category: 'southeast_asia' },
]

interface NewsArticle {
  title: string
  summary: string | null
  content: string | null
  source_url: string
  source_name: string
  category: string
  image_url: string | null
  published_at: string | null
}

// æ’é™¤çš„URLæ¨¡å¼ - ç™»å½•é¡µã€ç®¡ç†é¡µã€æ— å®é™…å†…å®¹çš„é¡µé¢
const EXCLUDED_URL_PATTERNS = [
  '/wp-login',
  '/wp-admin',
  '/login',
  '/signin',
  '/sign-in',
  '/register',
  '/signup',
  '/sign-up',
  '/admin',
  '/dashboard',
  '/account',
  '/cart',
  '/checkout',
  '/feed',
  '/rss',
  '/xmlrpc',
  '/wp-json',
  '/api/',
  '/search',
  '/tag/',
  '/category/',
  '/author/',
  '/page/',
  '/attachment/',
  '/comments/',
]

// æ’é™¤çš„æ ‡é¢˜å…³é”®è¯ - ç™»å½•ã€ç®¡ç†ç­‰éå†…å®¹é¡µé¢
const EXCLUDED_TITLE_PATTERNS = [
  'ç™»å½•',
  'ç™»é™†',
  'Login',
  'Sign In',
  'Sign Up',
  'æ³¨å†Œ',
  'WordPress',
  'Dashboard',
  'ç®¡ç†',
  'åå°',
  'Admin',
  'Error',
  'é”™è¯¯',
  '404',
  'æ‰¾ä¸åˆ°',
  'Not Found',
  'Password',
  'å¯†ç ',
  'Reset',
  'é‡ç½®',
]

// æ£€æŸ¥URLæ˜¯å¦åº”è¯¥è¢«æ’é™¤
function isExcludedUrl(url: string): boolean {
  const lowerUrl = url.toLowerCase()
  return EXCLUDED_URL_PATTERNS.some(pattern => lowerUrl.includes(pattern.toLowerCase()))
}

// æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åº”è¯¥è¢«æ’é™¤
function isExcludedTitle(title: string): boolean {
  if (!title) return true
  const lowerTitle = title.toLowerCase()
  return EXCLUDED_TITLE_PATTERNS.some(pattern => lowerTitle.includes(pattern.toLowerCase()))
}

// è¶…æ—¶æ§åˆ¶çš„fetch
async function fetchWithTimeout(url: string, options: RequestInit, timeoutMs = 15000): Promise<Response> {
  const controller = new AbortController()
  const timeout = setTimeout(() => controller.abort(), timeoutMs)
  
  try {
    const response = await fetch(url, {
      ...options,
      signal: controller.signal,
    })
    return response
  } finally {
    clearTimeout(timeout)
  }
}

// é‡è¯•æœºåˆ¶
async function fetchWithRetry(
  url: string, 
  options: RequestInit, 
  maxRetries = 2, 
  timeoutMs = 15000
): Promise<Response | null> {
  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      const response = await fetchWithTimeout(url, options, timeoutMs)
      if (response.ok) return response
      
      // å¦‚æœæ˜¯å®¢æˆ·ç«¯é”™è¯¯ï¼ˆ4xxï¼‰ï¼Œä¸é‡è¯•
      if (response.status >= 400 && response.status < 500) {
        console.log(`Client error ${response.status} for ${url}, not retrying`)
        return null
      }
      
      console.log(`Attempt ${attempt + 1} failed with status ${response.status}, retrying...`)
    } catch (error) {
      if (error instanceof Error && error.name === 'AbortError') {
        console.log(`Attempt ${attempt + 1} timed out for ${url}`)
      } else {
        console.log(`Attempt ${attempt + 1} error:`, error)
      }
    }
    
    // ç­‰å¾…åé‡è¯•
    if (attempt < maxRetries) {
      await new Promise(r => setTimeout(r, 1000 * (attempt + 1)))
    }
  }
  
  return null
}

// æ·±åº¦æŠ“å– cnmdnews.com å…¨éƒ¨æ–°é—»
async function crawlCnmdNews(
  firecrawlApiKey: string,
  category: string
): Promise<NewsArticle[]> {
  return crawlSiteDeep(firecrawlApiKey, 'https://www.cnmdnews.com/', 'CNMD News', category)
}

// ä¸“é—¨æŠ“å– weiruan.org å¯¼èˆªé“¾æ¥ - WeiRuanKeJiä¸“æ ï¼ˆèµ„æºæ¨èï¼‰
async function crawlWeiruanResources(
  firecrawlApiKey: string,
  category: string
): Promise<NewsArticle[]> {
  console.log('Fetching WeiRuanKeJi resources from weiruan.org...')
  
  try {
    // æŠ“å–å¯¼èˆªé¡µé¢å†…å®¹
    const response = await fetchWithRetry(
      'https://api.firecrawl.dev/v1/scrape',
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${firecrawlApiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          url: 'https://weiruan.org/',
          formats: ['markdown', 'links'],
          onlyMainContent: true,
        }),
      },
      2,
      20000
    )

    if (!response) {
      console.error('Failed to scrape weiruan.org')
      return []
    }

    let data
    try {
      data = await response.json()
    } catch (e) {
      console.error('Failed to parse weiruan.org response:', e)
      return []
    }

    const markdown = data.data?.markdown || data.markdown || ''
    const links = data.data?.links || data.links || []
    
    console.log(`Weiruan.org markdown length: ${markdown.length}, links: ${links.length}`)
    
    const articles: NewsArticle[] = []
    
    // å®šä¹‰åˆ†ç±»æ˜ å°„
    const categoryPatterns = [
      { name: 'å¨è½¯ç§‘æŠ€é›†ç¾¤', emoji: 'ğŸ”§' },
      { name: 'å¸¸ç”¨æ¨è', emoji: 'â­' },
      { name: 'è®ºå›', emoji: 'ğŸ’¬' },
      { name: 'VPSæœåŠ¡', emoji: 'ğŸ–¥ï¸' },
      { name: 'AI å·¥å…·', emoji: 'ğŸ¤–' },
      { name: 'é¢æ¿å·¥å…·', emoji: 'ğŸ“Š' },
      { name: 'å½±è§†èµ„æº', emoji: 'ğŸ¬' },
      { name: 'åŸŸåæœåŠ¡', emoji: 'ğŸŒ' },
      { name: 'é‚®ç®±', emoji: 'ğŸ“§' },
      { name: 'ç½‘ç›˜', emoji: 'â˜ï¸' },
      { name: 'å‹æƒ…é“¾æ¥', emoji: 'ğŸ”—' },
    ]
    
    // ä»markdownä¸­æå–èµ„æºæ¡ç›®
    // æ ¼å¼ç±»ä¼¼: **å¨è½¯è®¢é˜…** \n\n æš‚æ— æè¿°\n\n 49](https://www.weiruan.org/go/8)
    const resourcePattern = /\*\*([^*]+)\*\*\s*\\?\n?\\?\n?\s*([^\n\\]*?)\\?\n?\\?\n?\s*(\d+)\]\((https:\/\/www\.weiruan\.org\/go\/\d+)\)/g
    
    let match
    let currentCategory = 'å…¶ä»–èµ„æº'
    
    // å…ˆæŸ¥æ‰¾åˆ†ç±»æ ‡é¢˜
    const lines = markdown.split('\n')
    for (const line of lines) {
      // æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†ç±»æ ‡é¢˜
      for (const cat of categoryPatterns) {
        if (line.includes(cat.name)) {
          currentCategory = cat.name
          break
        }
      }
    }
    
    // æå–æ‰€æœ‰èµ„æº
    while ((match = resourcePattern.exec(markdown)) !== null) {
      const [, name, description, clicks, url] = match
      
      if (name && url) {
        const cleanName = name.trim()
        const cleanDesc = description.trim() === 'æš‚æ— æè¿°' ? '' : description.trim()
        
        // ä¸ºèµ„æºåˆ›å»ºæ ‡é¢˜ï¼ŒåŒ…å«æ¥æºåˆ†ç±»
        const title = `[èµ„æºæ¨è] ${cleanName}`
        const summary = cleanDesc || `å¨è½¯å¯¼èˆªæ¨èèµ„æº - çƒ­åº¦: ${clicks}`
        
        articles.push({
          title: title,
          summary: summary,
          content: `**${cleanName}**\n\n${cleanDesc || 'å¨è½¯å¯¼èˆªæ¨èèµ„æº'}\n\nçƒ­åº¦: ${clicks}\n\næ¥æº: weiruan.org`,
          source_url: url,
          source_name: 'WeiRuanKeJi',
          category: category,
          image_url: null,
          published_at: new Date().toISOString(),
        })
      }
    }
    
    // å¦‚æœæ­£åˆ™æ²¡æœ‰åŒ¹é…åˆ°ï¼Œå°è¯•å¦ä¸€ç§æ–¹å¼æå–
    if (articles.length === 0) {
      console.log('Trying alternative extraction method...')
      
      // ä»linksä¸­æå–weiruan.orgçš„è·³è½¬é“¾æ¥
      const weiruanLinks = links.filter((link: string) => 
        link.includes('weiruan.org/go/')
      )
      
      console.log(`Found ${weiruanLinks.length} weiruan.org redirect links`)
      
      // ä»markdownæå–åç§°
      const namePattern = /\*\*([^*]+)\*\*/g
      const names: string[] = []
      let nameMatch
      while ((nameMatch = namePattern.exec(markdown)) !== null) {
        const name = nameMatch[1].trim()
        if (name && name.length < 50 && !name.includes('\\')) {
          names.push(name)
        }
      }
      
      console.log(`Extracted ${names.length} resource names`)
      
      // åŒ¹é…åç§°å’Œé“¾æ¥
      for (let i = 0; i < Math.min(names.length, weiruanLinks.length); i++) {
        articles.push({
          title: `[èµ„æºæ¨è] ${names[i]}`,
          summary: `å¨è½¯å¯¼èˆªæ¨èèµ„æº`,
          content: `**${names[i]}**\n\nå¨è½¯å¯¼èˆªæ¨èèµ„æº\n\næ¥æº: weiruan.org`,
          source_url: weiruanLinks[i],
          source_name: 'WeiRuanKeJi',
          category: category,
          image_url: null,
          published_at: new Date().toISOString(),
        })
      }
    }
    
    // å»é‡
    const uniqueArticles = articles.filter((article, index, self) =>
      index === self.findIndex(a => a.title === article.title)
    )
    
    console.log(`Found ${uniqueArticles.length} WeiRuanKeJi resources`)
    return uniqueArticles
  } catch (error) {
    console.error('Error crawling weiruan.org:', error)
    return []
  }
}

// é€šç”¨æ·±åº¦æŠ“å–å‡½æ•° - å¢å¼ºç¨³å®šæ€§
async function crawlSiteDeep(
  firecrawlApiKey: string,
  siteUrl: string,
  sourceName: string,
  category: string
): Promise<NewsArticle[]> {
  console.log(`Deep crawling ${siteUrl} for ALL content...`)
  
  try {
    // é¦–å…ˆä½¿ç”¨ map API è·å–ç½‘ç«™æ‰€æœ‰URL
    console.log(`Step 1: Mapping ${siteUrl} to discover all URLs...`)
    const mapResponse = await fetchWithRetry(
      'https://api.firecrawl.dev/v1/map',
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${firecrawlApiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          url: siteUrl,
          limit: 2000, // å‡å°‘é™åˆ¶ä»¥æé«˜ç¨³å®šæ€§
          includeSubdomains: false,
        }),
      },
      2,
      20000
    )
    
    let urlsToScrape: string[] = []
    
    if (mapResponse) {
      try {
        const mapData = await mapResponse.json()
        urlsToScrape = mapData.data?.links || mapData.links || []
        console.log(`Found ${urlsToScrape.length} URLs from ${siteUrl}`)
      } catch (e) {
        console.error('Failed to parse map response:', e)
      }
    }
    
    // è¿‡æ»¤å‡ºæ–‡ç« URLï¼Œæ’é™¤ç™»å½•é¡µã€ç®¡ç†é¡µç­‰æ— å®é™…å†…å®¹çš„é¡µé¢
    const newsUrls = urlsToScrape.filter(url => {
      const isArticle = url.includes('/20') || 
                       url.includes('/news/') ||
                       url.includes('/article/') ||
                       url.includes('/post/') ||
                       url.includes('/blog/') ||
                       url.match(/\/\d{4}\/\d{2}\//)
      const isNotAsset = !url.match(/\.(jpg|png|gif|css|js|pdf)$/i)
      // æ’é™¤éå†…å®¹é¡µé¢
      const isNotExcluded = !isExcludedUrl(url)
      return isArticle && isNotAsset && isNotExcluded
    })
    
    console.log(`Filtered to ${newsUrls.length} potential article URLs`)
    
    // Step 2: ä½¿ç”¨ crawl API æ·±åº¦æŠ“å– - å‡å°‘é…ç½®ä»¥æé«˜ç¨³å®šæ€§
    console.log(`Step 2: Starting crawl job for ${siteUrl}...`)
    const crawlStartResponse = await fetchWithRetry(
      'https://api.firecrawl.dev/v1/crawl',
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${firecrawlApiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          url: siteUrl,
          limit: 100, // å‡å°‘é™åˆ¶ä»¥æé«˜ç¨³å®šæ€§
          maxDepth: 3, // å‡å°‘æ·±åº¦
          scrapeOptions: {
            formats: ['markdown'],
            onlyMainContent: true,
          },
        }),
      },
      2,
      30000
    )

    if (!crawlStartResponse) {
      console.error(`Failed to start crawl for ${siteUrl}`)
      return scrapeNewsFromUrl(firecrawlApiKey, siteUrl, sourceName, category)
    }

    let crawlStartData
    try {
      crawlStartData = await crawlStartResponse.json()
      console.log('Crawl response:', JSON.stringify(crawlStartData).substring(0, 300))
    } catch (e) {
      console.error('Failed to parse crawl start response:', e)
      return scrapeNewsFromUrl(firecrawlApiKey, siteUrl, sourceName, category)
    }

    const crawlId = crawlStartData.id
    if (!crawlId) {
      console.error('Crawl did not return an id')
      return scrapeNewsFromUrl(firecrawlApiKey, siteUrl, sourceName, category)
    }

    const sleep = (ms: number) => new Promise((r) => setTimeout(r, ms))

    // è½®è¯¢æœ€å¤š ~30 ç§’
    let crawlResult: any = null
    for (let attempt = 0; attempt < 10; attempt++) {
      await sleep(3000)

      const statusResp = await fetchWithRetry(
        `https://api.firecrawl.dev/v1/crawl/${crawlId}`,
        {
          method: 'GET',
          headers: {
            'Authorization': `Bearer ${firecrawlApiKey}`,
            'Content-Type': 'application/json',
          },
        },
        1,
        10000
      )

      if (!statusResp) {
        console.error(`Failed to check crawl status`)
        continue
      }

      try {
        const statusData = await statusResp.json()
        const status = statusData.status || statusData.data?.status
        console.log(`Crawl status (attempt ${attempt + 1}/10): ${status}`)

        if (status === 'completed') {
          crawlResult = statusData
          break
        }

        if (status === 'failed' || status === 'cancelled') {
          console.error('Crawl failed:', JSON.stringify(statusData).substring(0, 300))
          break
        }
      } catch (e) {
        console.error('Failed to parse status response:', e)
      }
    }

    if (!crawlResult) {
      console.log('Crawl did not complete in time, falling back to scrape')
      return scrapeNewsFromUrl(firecrawlApiKey, siteUrl, sourceName, category)
    }

    const articles: NewsArticle[] = []

    // å¤„ç†çˆ¬å–ç»“æœ
    const pages = Array.isArray(crawlResult.data)
      ? crawlResult.data
      : Array.isArray(crawlResult.data?.data)
        ? crawlResult.data.data
        : []

    console.log(`Crawled ${pages.length} pages from ${siteUrl}`)
    
    for (const page of pages) {
      const markdown = page.markdown || page.data?.markdown || ''
      const metadata = page.metadata || page.data?.metadata || {}
      const sourceUrl = metadata.sourceURL || metadata.sourceUrl || page.url || siteUrl
      
      // æ£€æŸ¥URLå’Œæ ‡é¢˜æ˜¯å¦åº”è¯¥è¢«æ’é™¤
      if (isExcludedUrl(sourceUrl)) {
        console.log(`Skipping excluded URL: ${sourceUrl}`)
        continue
      }
      
      if (metadata.title && metadata.title.length > 10 && metadata.title.length < 300 && !isExcludedTitle(metadata.title)) {
        articles.push({
          title: metadata.title,
          summary: metadata.description || null,
          content: markdown.substring(0, 1000) || null,
          source_url: sourceUrl,
          source_name: sourceName,
          category: category,
          image_url: metadata.ogImage || null,
          published_at: new Date().toISOString(),
        })
      }
      
      // ä» markdown ä¸­æå–æ›´å¤šæ ‡é¢˜
      const titleMatches = markdown.match(/^#{1,3}\s+(.+)$/gm) || []
      for (const match of titleMatches.slice(0, 3)) {
        const title = match.replace(/^#+\s+/, '').trim()
        if (title.length >= 10 && title.length <= 300 && !isExcludedTitle(title)) {
          articles.push({
            title: title,
            summary: null,
            content: null,
            source_url: sourceUrl,
            source_name: sourceName,
            category: category,
            image_url: metadata.ogImage || null,
            published_at: new Date().toISOString(),
          })
        }
      }
    }
    
    // å»é‡
    const uniqueArticles = articles.filter((article, index, self) =>
      index === self.findIndex(a => a.title === article.title)
    )
    
    console.log(`Found ${uniqueArticles.length} unique articles from ${siteUrl}`)
    return uniqueArticles
  } catch (error) {
    console.error(`Error crawling ${siteUrl}:`, error)
    // å¤±è´¥æ—¶å°è¯•ç®€å•æŠ“å–
    return scrapeNewsFromUrl(firecrawlApiKey, siteUrl, sourceName, category)
  }
}

async function scrapeNewsFromUrl(
  firecrawlApiKey: string,
  url: string,
  sourceName: string,
  category: string
): Promise<NewsArticle[]> {
  console.log(`Scraping news from ${sourceName}: ${url}`)
  
  try {
    const response = await fetchWithRetry(
      'https://api.firecrawl.dev/v1/scrape',
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${firecrawlApiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          url,
          formats: ['markdown', 'links'],
          onlyMainContent: true,
        }),
      },
      2,
      15000
    )

    if (!response) {
      console.error(`Failed to scrape ${url} after retries`)
      return []
    }

    let data
    try {
      data = await response.json()
    } catch (e) {
      console.error(`Failed to parse response from ${url}:`, e)
      return []
    }
    
    const markdown = data.data?.markdown || data.markdown || ''
    const links = data.data?.links || data.links || []
    const metadata = data.data?.metadata || data.metadata || {}

    // ä» markdown å†…å®¹ä¸­æå–æ–°é—»æ ‡é¢˜å’Œæ‘˜è¦
    const articles: NewsArticle[] = []
    
    // ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯ ## æˆ– ### å¼€å¤´çš„è¡Œï¼‰
    const titleMatches = markdown.match(/^#{1,3}\s+(.+)$/gm) || []
    const uniqueTitles = [...new Set(titleMatches.map((t: string) => t.replace(/^#+\s+/, '').trim()))]
    
    // å–å‰10æ¡æ–°é—»
    const titlesToProcess = uniqueTitles.slice(0, 10) as string[]
    
    for (const title of titlesToProcess) {
      if (title.length < 10 || title.length > 300) continue
      
      // æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åº”è¯¥è¢«æ’é™¤
      if (isExcludedTitle(title)) {
        console.log(`Skipping excluded title: ${title}`)
        continue
      }
      
      // å°è¯•ä»é“¾æ¥ä¸­æ‰¾åˆ°å¯¹åº”çš„æ–‡ç« é“¾æ¥
      const matchedLink = links.find((link: string) => {
        const titleWords = title.toLowerCase().split(' ').filter((w: string) => w.length > 3)
        return titleWords.some((word: string) => link.toLowerCase().includes(word))
      })
      
      const articleUrl = matchedLink || url
      
      // æ£€æŸ¥URLæ˜¯å¦åº”è¯¥è¢«æ’é™¤
      if (isExcludedUrl(articleUrl)) {
        console.log(`Skipping excluded URL: ${articleUrl}`)
        continue
      }
      
      articles.push({
        title: title,
        summary: null,
        content: null,
        source_url: articleUrl,
        source_name: sourceName,
        category: category,
        image_url: metadata.ogImage || null,
        published_at: new Date().toISOString(),
      })
    }

    console.log(`Found ${articles.length} articles from ${sourceName}`)
    return articles
  } catch (error) {
    console.error(`Error scraping ${url}:`, error)
    return []
  }
}

Deno.serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders })
  }

  try {
    console.log('Fetch news request received')

    const firecrawlApiKey = Deno.env.get('FIRECRAWL_API_KEY')
    if (!firecrawlApiKey) {
      console.error('FIRECRAWL_API_KEY not configured')
      return new Response(
        JSON.stringify({ success: false, error: 'Firecrawl not configured' }),
        { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      )
    }

    const supabaseUrl = Deno.env.get('SUPABASE_URL')!
    const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!
    const supabase = createClient(supabaseUrl, supabaseServiceKey)

    // å¯é€‰ï¼šè·å–æŒ‡å®šçš„æ–°é—»æº
    let sourcesToScrape = NEWS_SOURCES
    try {
      const body = await req.json()
      if (body.sources && Array.isArray(body.sources)) {
        sourcesToScrape = NEWS_SOURCES.filter(s => body.sources.includes(s.name))
      }
      if (body.category) {
        sourcesToScrape = sourcesToScrape.filter(s => s.category === body.category)
      }
    } catch {
      // æ²¡æœ‰ bodyï¼Œä½¿ç”¨æ‰€æœ‰æº
    }

    console.log(`Starting to scrape ${sourcesToScrape.length} news sources`)

    const allArticles: NewsArticle[] = []
    const errors: string[] = []

    // ç‰¹æ®Šå¤„ç† cnmdnews.com - æ·±åº¦æŠ“å–
    const cnmdSource = sourcesToScrape.find(s => s.name === 'CNMD News')
    if (cnmdSource) {
      console.log('Deep crawling cnmdnews.com...')
      try {
        const cnmdArticles = await crawlCnmdNews(firecrawlApiKey, cnmdSource.category)
        allArticles.push(...cnmdArticles)
      } catch (e) {
        console.error('CNMD crawl failed:', e)
        errors.push('CNMD News')
      }
      sourcesToScrape = sourcesToScrape.filter(s => s.name !== 'CNMD News')
    }

    // ç‰¹æ®Šå¤„ç† weiruan.org - æŠ“å–å¯¼èˆªèµ„æº
    const weiruanSource = sourcesToScrape.find(s => s.name === 'WeiRuanKeJi')
    if (weiruanSource) {
      console.log('Fetching WeiRuanKeJi resources from weiruan.org...')
      try {
        const weiruanArticles = await crawlWeiruanResources(firecrawlApiKey, weiruanSource.category)
        allArticles.push(...weiruanArticles)
      } catch (e) {
        console.error('WeiRuanKeJi fetch failed:', e)
        errors.push('WeiRuanKeJi')
      }
      sourcesToScrape = sourcesToScrape.filter(s => s.name !== 'WeiRuanKeJi')
    }

    // å¹¶è¡Œçˆ¬å–å…¶ä»–æ–°é—»æºï¼ˆé™åˆ¶å¹¶å‘æ•°ï¼‰- å‡å°‘æ‰¹æ¬¡å¤§å°ä»¥æé«˜ç¨³å®šæ€§
    const batchSize = 2
    for (let i = 0; i < sourcesToScrape.length; i += batchSize) {
      const batch = sourcesToScrape.slice(i, i + batchSize)
      const results = await Promise.allSettled(
        batch.map(source => 
          scrapeNewsFromUrl(firecrawlApiKey, source.url, source.name, source.category)
        )
      )
      
      for (let j = 0; j < results.length; j++) {
        const result = results[j]
        if (result.status === 'fulfilled') {
          allArticles.push(...result.value)
        } else {
          console.error(`Failed to scrape ${batch[j].name}:`, result.reason)
          errors.push(batch[j].name)
        }
      }
    }

    console.log(`Total articles scraped: ${allArticles.length}`)
    if (errors.length > 0) {
      console.log(`Sources with errors: ${errors.join(', ')}`)
    }

    // æ’å…¥æ–°é—»åˆ°æ•°æ®åº“ï¼ˆæ‰¹é‡upsertä»¥æé«˜æ•ˆç‡ï¼‰
    let insertedCount = 0
    const batchInsertSize = 50
    
    for (let i = 0; i < allArticles.length; i += batchInsertSize) {
      const batch = allArticles.slice(i, i + batchInsertSize)
      
      const { error, count } = await supabase
        .from('news_articles')
        .upsert(batch, { 
          onConflict: 'source_url',
          ignoreDuplicates: true 
        })
      
      if (!error) {
        insertedCount += batch.length
      } else {
        console.error('Batch insert error:', error.message)
      }
    }

    console.log(`Inserted ${insertedCount} articles (including updates)`)

    return new Response(
      JSON.stringify({ 
        success: true, 
        scraped: allArticles.length,
        inserted: insertedCount,
        errors: errors.length > 0 ? errors : undefined
      }),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    )
  } catch (error) {
    console.error('Error in fetch-news function:', error)
    return new Response(
      JSON.stringify({ 
        success: false, 
        error: error instanceof Error ? error.message : 'Unknown error',
        partial: true // è¡¨ç¤ºéƒ¨åˆ†æˆåŠŸ
      }),
      { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    )
  }
})